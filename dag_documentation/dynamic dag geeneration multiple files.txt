Generate DAGs From JSON Config Files
   One way of implementing a multiple-file method is using a Python script 
   to generate DAG files based on a set of JSON configuration files. 
   For this simple example, we will assume that all DAGs will have the same structure: 
   each will have a single task that uses the PostgresOperator to execute a query. 
   This use case might be relevant for a team of analysts who need to schedule SQL queries,
   where the DAG is mostly the same but the query and the schedule are changing.

   To start, we will create a DAG 'template' file that defines the DAG's structure. 
   This looks just like a regular DAG file, but we have added specific variables 
   where we know information is going to be dynamically generated ie dag_id, scheduletoreplace, and querytoreplace.
   Example 
      from airflow import DAG
      from airflow.operators.postgres_operator import PostgresOperator
      from datetime import datetime

      default_args = {'owner': 'airflow',
                      'start_date': datetime(2021, 1, 1)
                      }
      dag = DAG(dag_id,
                  schedule_interval=scheduletoreplace,
                  default_args=default_args,
                  catchup=False)

      with dag:
         t1 = PostgresOperator(
              task_id='postgres_query',
              postgres_conn_id=connection_id
              sql=querytoreplace)

      # Next we create a dag-config folder that will contain a JSON config file 
      # for each DAG. The config file should define the parameters that we noted above, 
      # the DAG Id, schedule interval, and query to be executed.

      {
         "DagId": "dag_file_1",
         "Schedule": "'@daily'",
         "Query":"'SELECT * FROM table1;'"
      }

      # Finally, we create a Python script that will create the DAG files based on
      # the template and the config files. The script loops through every config file
      # in the dag-config/ folder, makes a copy of the template in the dags/ folder, 
      # and overwrites the parameters in that file with the ones from the config file.

      import json
      import os
      import shutil
      import fileinput

      config_filepath = 'include/dag-config/'
      dag_template_filename = 'include/dag-template.py'

      for filename in os.listdir(config_filepath):
         f = open(filepath + filename)
         config = json.load(f)
    
         new_filename = 'dags/'+config['DagId']+'.py'
         shutil.copyfile(dag_template_filename, new_filename)
    

         for line in fileinput.input(new_filename, inplace=True):
            line.replace("dag_id", "'"+config['DagId']+"'")
            line.replace("scheduletoreplace", config['Schedule'])
            line.replace("querytoreplace", config['Query'])
            print(line, end="")
      # Now to generate our DAG files, we can either run this script ad-hoc 
      # or as part of our CI/CD workflow. After running the script, 
      # our final directory would look like the example below, 
      # where the include/ directory contains the files shown above, and the dags/ directory 
      contains the two dynamically generated DAGs:
      dags/
      ├── dag_file_1.py
      ├── dag_file_2.py 
      include/
      ├── dag-template.py
      ├── generate-dag-files.py
      └── dag-config
         ├── dag1-config.json
         └── dag2-config.json
      This works only if all of the DAGs follow the same pattern. 
      However, it could be expanded to have dynamic inputs for tasks, dependencies, operators, etc.


